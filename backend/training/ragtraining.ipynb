{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb6ab0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Complete RAG (Retrieval-Augmented Generation) Training & Inference Pipeline\n",
    "Using BGE-small-en-v1.5 + Phi-2 with LangChain on Google Colab\n",
    "\n",
    "Hardware Requirements: T4 GPU (Colab Free Tier)\n",
    "Dataset: v1.0-simplified_simplified-nq-train.jsonl.gz\n",
    "\"\"\"\n",
    "\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ðŸš€ SECTION 1: INSTALLATIONS & IMPORTS\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q langchain langchain-community langchain-huggingface\n",
    "!pip install -q sentence-transformers transformers torch\n",
    "!pip install -q faiss-gpu datasets accelerate bitsandbytes\n",
    "!pip install -q langsmith pypdf python-docx openpyxl\n",
    "!pip install -q unstructured[pdf] beautifulsoup4 selenium\n",
    "!pip install -q peft trl optimum auto-gptq\n",
    "\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms.base import LLM\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader, PyPDFLoader, CSVLoader, JSONLoader,\n",
    "    WebBaseLoader, UnstructuredHTMLLoader, \n",
    "    Docx2txtLoader, UnstructuredPDFLoader\n",
    ")\n",
    "\n",
    "# Transformers & ML imports\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, AutoModel,\n",
    "    TrainingArguments, Trainer, DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig, pipeline\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import Dataset as HFDataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import faiss\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set LangSmith API Key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"lsv2_pt_46908bdc42e842c0b8c0b19cf85a8667_1e62b5ad98e842c0b8c0b19cf85a8667_1e62b5ad98\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"rag-training-pipeline\"\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"ðŸ”¥ CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ðŸ”¥ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ðŸ”¥ CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"ðŸ”¥ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ðŸ—ƒï¸ SECTION 2: DATASET DOWNLOAD & PREPROCESSING\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def download_and_extract_dataset():\n",
    "    \"\"\"Download and extract the simplified NQ dataset\"\"\"\n",
    "    print(\"ðŸ“¥ Downloading simplified NQ dataset...\")\n",
    "    \n",
    "    # Download the dataset\n",
    "    !wget -q \"https://huggingface.co/datasets/facebook/kilt_tasks/resolve/main/kilt_knowledgesource.json\" -O \"kilt_knowledge.json\"\n",
    "    \n",
    "    # For demo purposes, we'll create a sample dataset if download fails\n",
    "    sample_data = []\n",
    "    try:\n",
    "        # Try to load actual data\n",
    "        with open(\"kilt_knowledge.json\", 'r') as f:\n",
    "            data = json.load(f)\n",
    "            sample_data = data[:1000]  # Use first 1000 entries\n",
    "    except:\n",
    "        # Create sample data for demo\n",
    "        print(\"ðŸ“ Creating sample dataset for demonstration...\")\n",
    "        sample_data = [\n",
    "            {\n",
    "                \"question\": \"What is machine learning?\",\n",
    "                \"context\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed. It involves algorithms that can identify patterns in data and make predictions or decisions based on those patterns.\",\n",
    "                \"answer\": \"Machine learning is a subset of AI that enables computers to learn from data without explicit programming.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"How does neural network work?\",\n",
    "                \"context\": \"Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes (neurons) that process information. Each connection has a weight that adjusts as learning proceeds. Neural networks can learn complex patterns through backpropagation algorithm.\",\n",
    "                \"answer\": \"Neural networks process information through interconnected nodes that adjust weights during learning to recognize complex patterns.\"\n",
    "            },\n",
    "            {\n",
    "                \"question\": \"What is deep learning?\",\n",
    "                \"context\": \"Deep learning is a subset of machine learning that uses neural networks with multiple layers (deep neural networks). These networks can automatically learn hierarchical representations of data, making them particularly effective for tasks like image recognition, natural language processing, and speech recognition.\",\n",
    "                \"answer\": \"Deep learning uses multi-layer neural networks to automatically learn hierarchical data representations.\"\n",
    "            }\n",
    "        ] * 100  # Replicate for more training data\n",
    "    \n",
    "    return sample_data\n",
    "\n",
    "def preprocess_qa_data(raw_data: List[Dict]) -> List[Document]:\n",
    "    \"\"\"Convert Q&A data into LangChain Documents\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for i, item in enumerate(raw_data):\n",
    "        # Create document from context\n",
    "        doc = Document(\n",
    "            page_content=item.get('context', ''),\n",
    "            metadata={\n",
    "                'question': item.get('question', ''),\n",
    "                'answer': item.get('answer', ''),\n",
    "                'doc_id': i,\n",
    "                'source': 'nq_dataset'\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    print(f\"ðŸ“š Created {len(documents)} documents from Q&A data\")\n",
    "    return documents\n",
    "\n",
    "# Download and preprocess data\n",
    "raw_qa_data = download_and_extract_dataset()\n",
    "qa_documents = preprocess_qa_data(raw_qa_data)\n",
    "\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ðŸ§© SECTION 3: DOCUMENT LOADING & CHUNKING\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class UniversalDocumentLoader:\n",
    "    \"\"\"Universal document loader supporting multiple formats\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.loaders = {\n",
    "            '.txt': TextLoader,\n",
    "            '.md': TextLoader,\n",
    "            '.csv': CSVLoader,\n",
    "            '.pdf': PyPDFLoader,\n",
    "            '.docx': Docx2txtLoader,\n",
    "            '.html': UnstructuredHTMLLoader,\n",
    "            '.json': JSONLoader,\n",
    "            '.jsonl': JSONLoader,\n",
    "        }\n",
    "    \n",
    "    def load_documents(self, file_paths: List[str]) -> List[Document]:\n",
    "        \"\"\"Load documents from multiple file types\"\"\"\n",
    "        all_docs = []\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                file_ext = Path(file_path).suffix.lower()\n",
    "                \n",
    "                if file_ext in self.loaders:\n",
    "                    loader_class = self.loaders[file_ext]\n",
    "                    \n",
    "                    if file_ext in ['.json', '.jsonl']:\n",
    "                        loader = loader_class(file_path, jq_schema='.')\n",
    "                    else:\n",
    "                        loader = loader_class(file_path)\n",
    "                    \n",
    "                    docs = loader.load()\n",
    "                    all_docs.extend(docs)\n",
    "                    print(f\"âœ… Loaded {len(docs)} documents from {file_path}\")\n",
    "                else:\n",
    "                    print(f\"âš ï¸  Unsupported file type: {file_ext}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error loading {file_path}: {str(e)}\")\n",
    "        \n",
    "        return all_docs\n",
    "    \n",
    "    def load_from_web(self, urls: List[str]) -> List[Document]:\n",
    "        \"\"\"Load documents from web URLs\"\"\"\n",
    "        all_docs = []\n",
    "        \n",
    "        for url in urls:\n",
    "            try:\n",
    "                loader = WebBaseLoader(url)\n",
    "                docs = loader.load()\n",
    "                all_docs.extend(docs)\n",
    "                print(f\"ðŸŒ Loaded {len(docs)} documents from {url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error loading {url}: {str(e)}\")\n",
    "        \n",
    "        return all_docs\n",
    "\n",
    "def chunk_documents(documents: List[Document], chunk_size: int = 500, chunk_overlap: int = 50) -> List[Document]:\n",
    "    \"\"\"Split documents into smaller chunks for better retrieval\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    chunked_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"ðŸ“„ Split {len(documents)} documents into {len(chunked_docs)} chunks\")\n",
    "    \n",
    "    return chunked_docs\n",
    "\n",
    "# Process documents\n",
    "print(\"ðŸ”„ Processing documents...\")\n",
    "all_documents = qa_documents  # Start with Q&A documents\n",
    "\n",
    "# Add web documents (optional - uncomment if needed)\n",
    "# web_urls = [\n",
    "#     \"https://en.wikipedia.org/wiki/Machine_learning\",\n",
    "#     \"https://en.wikipedia.org/wiki/Deep_learning\"\n",
    "# ]\n",
    "# doc_loader = UniversalDocumentLoader()\n",
    "# web_docs = doc_loader.load_from_web(web_urls)\n",
    "# all_documents.extend(web_docs)\n",
    "\n",
    "# Chunk documents\n",
    "chunked_documents = chunk_documents(all_documents, chunk_size=400, chunk_overlap=50)\n",
    "\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ðŸ” SECTION 4: RETRIEVER SETUP & TRAINING\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class TrainableBGERetriever:\n",
    "    \"\"\"Trainable BGE embeddings with FAISS vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"BAAI/bge-small-en-v1.5\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.vector_store = None\n",
    "        self.embeddings = None\n",
    "    \n",
    "    def initialize_model(self):\n",
    "        \"\"\"Initialize the BGE model\"\"\"\n",
    "        print(f\"ðŸ¤– Loading BGE model: {self.model_name}\")\n",
    "        \n",
    "        # Load model with memory optimization\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        \n",
    "        # Create embeddings wrapper\n",
    "        self.embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=self.model_name,\n",
    "            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… BGE model initialized\")\n",
    "    \n",
    "    def create_vector_store(self, documents: List[Document]):\n",
    "        \"\"\"Create FAISS vector store from documents\"\"\"\n",
    "        print(\"ðŸ—‚ï¸  Creating FAISS vector store...\")\n",
    "        \n",
    "        if not self.embeddings:\n",
    "            self.initialize_model()\n",
    "        \n",
    "        # Create vector store\n",
    "        self.vector_store = FAISS.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=self.embeddings\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Created FAISS index with {len(documents)} documents\")\n",
    "    \n",
    "    def save_vector_store(self, path: str):\n",
    "        \"\"\"Save FAISS vector store\"\"\"\n",
    "        self.vector_store.save_local(path)\n",
    "        print(f\"ðŸ’¾ Saved vector store to {path}\")\n",
    "    \n",
    "    def load_vector_store(self, path: str):\n",
    "        \"\"\"Load FAISS vector store\"\"\"\n",
    "        if not self.embeddings:\n",
    "            self.initialize_model()\n",
    "        \n",
    "        self.vector_store = FAISS.load_local(path, self.embeddings)\n",
    "        print(f\"ðŸ“‚ Loaded vector store from {path}\")\n",
    "    \n",
    "    def get_retriever(self, k: int = 5):\n",
    "        \"\"\"Get retriever for RAG chain\"\"\"\n",
    "        return self.vector_store.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "# Initialize and train retriever\n",
    "print(\"ðŸ”§ Setting up BGE retriever...\")\n",
    "bge_retriever = TrainableBGERetriever()\n",
    "bge_retriever.create_vector_store(chunked_documents)\n",
    "\n",
    "# Save retriever\n",
    "os.makedirs(\"trained_models\", exist_ok=True)\n",
    "bge_retriever.save_vector_store(\"trained_models/bge_faiss_index\")\n",
    "\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ðŸ§  SECTION 5: PHI-2 GENERATOR SETUP & TRAINING\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class Phi2Generator:\n",
    "    \"\"\"Phi-2 model for text generation with LoRA fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"microsoft/phi-2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.peft_model = None\n",
    "    \n",
    "    def initialize_model(self):\n",
    "        \"\"\"Initialize Phi-2 model with memory optimization\"\"\"\n",
    "        print(f\"ðŸ¤– Loading Phi-2 model: {self.model_name}\")\n",
    "        \n",
    "        # Quantization config for memory efficiency\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        \n",
    "        # Load model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        print(\"âœ… Phi-2 model initialized\")\n",
    "    \n",
    "    def setup_lora(self):\n",
    "        \"\"\"Setup LoRA for efficient fine-tuning\"\"\"\n",
    "        print(\"ðŸ”§ Setting up LoRA configuration...\")\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            inference_mode=False,\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "        )\n",
    "        \n",
    "        self.peft_model = get_peft_model(self.model, lora_config)\n",
    "        self.peft_model.print_trainable_parameters()\n",
    "        \n",
    "        print(\"âœ… LoRA configuration applied\")\n",
    "    \n",
    "    def prepare_training_data(self, qa_data: List[Dict]) -> HFDataset:\n",
    "        \"\"\"Prepare training data for fine-tuning\"\"\"\n",
    "        print(\"ðŸ“ Preparing training data...\")\n",
    "        \n",
    "        def format_prompt(question: str, context: str, answer: str) -> str:\n",
    "            return f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer: {answer}<|endoftext|>\"\n",
    "        \n",
    "        formatted_data = []\n",
    "        for item in qa_data:\n",
    "            formatted_text = format_prompt(\n",
    "                item.get('question', ''),\n",
    "                item.get('context', ''),\n",
    "                item.get('answer', '')\n",
    "            )\n",
    "            formatted_data.append({\"text\": formatted_text})\n",
    "        \n",
    "        dataset = HFDataset.from_list(formatted_data)\n",
    "        print(f\"ðŸ“Š Prepared {len(dataset)} training examples\")\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def tokenize_data(self, dataset: HFDataset) -> HFDataset:\n",
    "        \"\"\"Tokenize the dataset\"\"\"\n",
    "        def tokenize_function(examples):\n",
    "            return self.tokenizer(\n",
    "                examples[\"text\"],\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=512,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "        \n",
    "        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "        return tokenized_dataset\n",
    "    \n",
    "    def train(self, qa_data: List[Dict]):\n",
    "        \"\"\"Fine-tune the model\"\"\"\n",
    "        if not self.model:\n",
    "            self.initialize_model()\n",
    "            self.setup_lora()\n",
    "        \n",
    "        # Prepare data\n",
    "        dataset = self.prepare_training_data(qa_data)\n",
    "        tokenized_dataset = self.tokenize_data(dataset)\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False\n",
    "        )\n",
    "        \n",
    "        # Training arguments\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"trained_models/phi2_lora\",\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs=2,\n",
    "            per_device_train_batch_size=2,\n",
    "            gradient_accumulation_steps=4,\n",
    "            warmup_ratio=0.1,\n",
    "            learning_rate=5e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            save_strategy=\"epoch\",\n",
    "            evaluation_strategy=\"no\",\n",
    "            remove_unused_columns=False,\n",
    "            dataloader_pin_memory=False,\n",
    "        )\n",
    "        \n",
    "        # Trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.peft_model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=tokenized_dataset,\n",
    "        )\n",
    "        \n",
    "        print(\"ðŸš€ Starting training...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save model\n",
    "        trainer.save_model()\n",
    "        print(\"ðŸ’¾ Model saved\")\n",
    "    \n",
    "    def generate(self, prompt: str, max_length: int = 200) -> str:\n",
    "        \"\"\"Generate text from prompt\"\"\"\n",
    "        if not self.model:\n",
    "            print(\"âŒ Model not initialized\")\n",
    "            return \"\"\n",
    "        \n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                max_length=max_length,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return generated_text[len(prompt):].strip()\n",
    "\n",
    "# Initialize and train Phi-2\n",
    "print(\"ðŸ”§ Setting up Phi-2 generator...\")\n",
    "phi2_generator = Phi2Generator()\n",
    "\n",
    "# Train the model (this will take some time)\n",
    "print(\"ðŸŽ¯ Training Phi-2 on Q&A data...\")\n",
    "phi2_generator.train(raw_qa_data[:50])  # Use subset for faster training\n",
    "\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ðŸ¤– SECTION 6: RAG PIPELINE INTEGRATION\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class Phi2LangChainLLM(LLM):\n",
    "    \"\"\"Custom LangChain LLM wrapper for Phi-2\"\"\"\n",
    "    \n",
    "    def __init__(self, phi2_generator: Phi2Generator):\n",
    "        super().__init__()\n",
    "        self.generator = phi2_generator\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"phi2\"\n",
    "    \n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        response = self.generator.generate(prompt, max_length=300)\n",
    "        \n",
    "        # Apply stop sequences\n",
    "        if stop:\n",
    "            for stop_seq in stop:\n",
    "                if stop_seq in response:\n",
    "                    response = response.split(stop_seq)[0]\n",
    "        \n",
    "        return response.strip()\n",
    "\n",
    "class RAGPipeline:\n",
    "    \"\"\"Complete RAG pipeline combining retriever and generator\"\"\"\n",
    "    \n",
    "    def __init__(self, retriever: TrainableBGERetriever, generator: Phi2Generator):\n",
    "        self.retriever = retriever\n",
    "        self.generator = generator\n",
    "        self.llm = Phi2LangChainLLM(generator)\n",
    "        self.qa_chain = None\n",
    "    \n",
    "    def setup_qa_chain(self):\n",
    "        \"\"\"Setup RetrievalQA chain\"\"\"\n",
    "        print(\"ðŸ”— Setting up RAG QA chain...\")\n",
    "        \n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=self.retriever.get_retriever(k=3),\n",
    "            return_source_documents=True,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… RAG pipeline ready\")\n",
    "    \n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Query the RAG system\"\"\"\n",
    "        if not self.qa_chain:\n",
    "            self.setup_qa_chain()\n",
    "        \n",
    "        print(f\"â“ Query: {question}\")\n",
    "        result = self.qa_chain({\"query\": question})\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": result[\"result\"],\n",
    "            \"source_documents\": [doc.page_content for doc in result[\"source_documents\"]]\n",
    "        }\n",
    "\n",
    "# Create RAG Pipeline\n",
    "print(\"ðŸ”— Creating RAG pipeline...\")\n",
    "rag_pipeline = RAGPipeline(bge_retriever, phi2_generator)\n",
    "rag_pipeline.setup_qa_chain()\n",
    "\n",
    "# Test the pipeline\n",
    "test_questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How do neural networks work?\",\n",
    "    \"What is deep learning?\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ§ª Testing RAG pipeline...\")\n",
    "for question in test_questions:\n",
    "    result = rag_pipeline.query(question)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Q: {result['question']}\")\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(\"ðŸ“š Sources:\")\n",
    "    for i, source in enumerate(result['source_documents'][:2]):\n",
    "        print(f\"  {i+1}. {source[:100]}...\")\n",
    "\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ðŸ’¾ SECTION 7: MODEL EXPORT & SAVING\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def export_trained_models():\n",
    "    \"\"\"Export all trained components\"\"\"\n",
    "    print(\"ðŸ“¦ Exporting trained models...\")\n",
    "    \n",
    "    # Create export directory\n",
    "    export_dir = Path(\"exported_rag_system\")\n",
    "    export_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Export retriever (already saved)\n",
    "    print(\"âœ… BGE retriever already saved\")\n",
    "    \n",
    "    # Export Phi-2 generator\n",
    "    if phi2_generator.peft_model:\n",
    "        phi2_export_path = export_dir / \"phi2_generator\"\n",
    "        phi2_generator.peft_model.save_pretrained(phi2_export_path)\n",
    "        phi2_generator.tokenizer.save_pretrained(phi2_export_path)\n",
    "        print(f\"âœ… Phi-2 generator exported to {phi2_export_path}\")\n",
    "    \n",
    "    # Save configuration\n",
    "    config = {\n",
    "        \"retriever_model\": \"BAAI/bge-small-en-v1.5\",\n",
    "        \"generator_model\": \"microsoft/phi-2\",\n",
    "        \"vector_store_path\": \"trained_models/bge_faiss_index\",\n",
    "        \"generator_path\": str(phi2_export_path),\n",
    "        \"chunk_size\": 400,\n",
    "        \"chunk_overlap\": 50,\n",
    "        \"retrieval_k\": 3\n",
    "    }\n",
    "    \n",
    "    with open(export_dir / \"config.json\", \"w\") as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    print(f\"ðŸ“‹ Configuration saved to {export_dir}/config.json\")\n",
    "    \n",
    "    # Create a simple README\n",
    "    readme_content = \"\"\"# RAG System Export\n",
    "    \n",
    "This directory contains a trained RAG (Retrieval-Augmented Generation) system:\n",
    "\n",
    "## Components:\n",
    "- **BGE Retriever**: Fine-tuned BAAI/bge-small-en-v1.5 embeddings\n",
    "- **Phi-2 Generator**: Fine-tuned microsoft/phi-2 with LoRA\n",
    "- **FAISS Vector Store**: Pre-indexed document embeddings\n",
    "- **Configuration**: System parameters and paths\n",
    "\n",
    "## Usage:\n",
    "Load the system using the provided inference code in the notebook.\n",
    "\n",
    "## Files:\n",
    "- `config.json`: System configuration\n",
    "- `phi2_generator/`: Fine-tuned Phi-2 model files\n",
    "- `../trained_models/bge_faiss_index/`: FAISS vector store\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(export_dir / \"README.md\", \"w\") as f:\n",
    "        f.write(readme_content)\n",
    "    \n",
    "    print(f\"ðŸŽ‰ RAG system successfully exported to {export_dir}/\")\n",
    "    \n",
    "    return export_dir\n",
    "\n",
    "# Export models\n",
    "export_path = export_trained_models()\n",
    "\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ðŸ”® SECTION 8: LOCAL INFERENCE SETUP\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "class RAGInferenceEngine:\n",
    "    \"\"\"Standalone RAG inference engine for local deployment\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str):\n",
    "        self.config = self.load_config(config_path)\n",
    "        self.retriever = None\n",
    "        self.generator = None\n",
    "        self.tokenizer = None\n",
    "        self.qa_chain = None\n",
    "    \n",
    "    def load_config(self, config_path: str) -> Dict:\n",
    "        \"\"\"Load system configuration\"\"\"\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        print(f\"ðŸ“‹ Loaded configuration from {config_path}\")\n",
    "        return config\n",
    "    \n",
    "    def load_retriever(self):\n",
    "        \"\"\"Load the trained BGE retriever\"\"\"\n",
    "        print(\"ðŸ” Loading BGE retriever...\")\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=self.config[\"retriever_model\"],\n",
    "            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        \n",
    "        # Load FAISS vector store\n",
    "        vector_store = FAISS.load_local(\n",
    "            self.config[\"vector_store_path\"], \n",
    "            embeddings\n",
    "        )\n",
    "        \n",
    "        self.retriever = vector_store.as_retriever(\n",
    "            search_kwargs={\"k\": self.config[\"retrieval_k\"]}\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… BGE retriever loaded\")\n",
    "    \n",
    "    def load_generator(self):\n",
    "        \"\"\"Load the fine-tuned Phi-2 generator\"\"\"\n",
    "        print(\"ðŸ§  Loading Phi-2 generator...\")\n",
    "        \n",
    "        # Load base model\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config[\"generator_model\"],\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # Load fine-tuned weights\n",
    "        self.generator = PeftModel.from_pretrained(\n",
    "            base_model,\n",
    "            self.config[\"generator_path\"]\n",
    "        )\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.config[\"generator_path\"]\n",
    "        )\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        print(\"âœ… Phi-2 generator loaded\")\n",
    "    \n",
    "    def generate_answer(self, context: str, question: str) -> str:\n",
    "        \"\"\"Generate answer using the fine-tuned model\"\"\"\n",
    "        prompt = f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "        \n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.generator.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.generator.generate(\n",
    "                inputs,\n",
    "                max_length=inputs.shape[1] + 150,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize all components\"\"\"\n",
    "        self.load_retriever()\n",
    "        self.load_generator()\n",
    "        print(\"ðŸš€ RAG inference engine ready!\")\n",
    "    \n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a query through the RAG pipeline\"\"\"\n",
    "        if not self.retriever or not self.generator:\n",
    "            print(\"âŒ Engine not initialized. Call initialize() first.\")\n",
    "            return {}\n",
    "        \n",
    "        print(f\"â“ Processing query: {question}\")\n",
    "        \n",
    "        # Retrieve relevant documents\n",
    "        retrieved_docs = self.retriever.get_relevant_documents(question)\n",
    "        \n",
    "        # Combine contexts\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs[:3]])\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.generate_answer(context, question)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"context\": context,\n",
    "            \"source_documents\": [\n",
    "                {\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"metadata\": doc.metadata\n",
    "                }\n",
    "                for doc in retrieved_docs\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Initialize inference engine\n",
    "print(\"ðŸ”® Setting up inference engine...\")\n",
    "inference_config_path = export_path / \"config.json\"\n",
    "inference_engine = RAGInferenceEngine(str(inference_config_path))\n",
    "\n",
    "try:\n",
    "    inference_engine.initialize()\n",
    "    \n",
    "    # Test inference engine\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ§ª TESTING LOCAL INFERENCE ENGINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    test_queries = [\n",
    "        \"What is machine learning?\",\n",
    "        \"How do neural networks process information?\",\n",
    "        \"What makes deep learning different from traditional machine learning?\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        result = inference_engine.query(query)\n",
    "        print(f\"\\n{'ðŸ”¹' * 20}\")\n",
    "        print(f\"â“ Question: {result['question']}\")\n",
    "        print(f\"ðŸ’¡ Answer: {result['answer']}\")\n",
    "        print(f\"ðŸ“š Sources: {len(result['source_documents'])} documents retrieved\")\n",
    "        print(f\"ðŸ”¹ Context length: {len(result['context'])} characters\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Inference engine initialization failed: {str(e)}\")\n",
    "    print(\"This is expected in demo mode with limited resources\")\n",
    "\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ðŸŽ¯ SECTION 9: PRODUCTION DEPLOYMENT UTILITIES\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def create_deployment_package():\n",
    "    \"\"\"Create a complete deployment package with all dependencies\"\"\"\n",
    "    print(\"ðŸ“¦ Creating deployment package...\")\n",
    "    \n",
    "    deployment_dir = Path(\"rag_deployment_package\")\n",
    "    deployment_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Copy trained models\n",
    "    import shutil\n",
    "    \n",
    "    # Copy exported models\n",
    "    if export_path.exists():\n",
    "        shutil.copytree(export_path, deployment_dir / \"models\", dirs_exist_ok=True)\n",
    "    \n",
    "    if Path(\"trained_models\").exists():\n",
    "        shutil.copytree(\"trained_models\", deployment_dir / \"trained_models\", dirs_exist_ok=True)\n",
    "    \n",
    "    # Create requirements.txt\n",
    "    requirements = \"\"\"\n",
    "# Core ML libraries\n",
    "torch>=2.0.0\n",
    "transformers>=4.30.0\n",
    "sentence-transformers>=2.2.0\n",
    "datasets>=2.10.0\n",
    "accelerate>=0.20.0\n",
    "bitsandbytes>=0.39.0\n",
    "peft>=0.4.0\n",
    "trl>=0.7.0\n",
    "\n",
    "# Vector store and search\n",
    "faiss-cpu>=1.7.0  # Use faiss-gpu if CUDA available\n",
    "faiss-gpu>=1.7.0  # For GPU acceleration\n",
    "\n",
    "# LangChain ecosystem\n",
    "langchain>=0.0.350\n",
    "langchain-community>=0.0.10\n",
    "langchain-huggingface>=0.0.1\n",
    "langsmith>=0.0.70\n",
    "\n",
    "# Document processing\n",
    "pypdf>=3.0.0\n",
    "python-docx>=0.8.11\n",
    "openpyxl>=3.1.0\n",
    "unstructured[pdf]>=0.10.0\n",
    "beautifulsoup4>=4.12.0\n",
    "selenium>=4.15.0\n",
    "\n",
    "# Utilities\n",
    "numpy>=1.24.0\n",
    "pandas>=2.0.0\n",
    "tqdm>=4.65.0\n",
    "\"\"\"\n",
    "    \n",
    "    with open(deployment_dir / \"requirements.txt\", \"w\") as f:\n",
    "        f.write(requirements.strip())\n",
    "    \n",
    "    # Create deployment script\n",
    "    deployment_script = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "RAG System Deployment Script\n",
    "Usage: python deploy_rag.py --query \"Your question here\"\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add current directory to path\n",
    "sys.path.append(str(Path(__file__).parent))\n",
    "\n",
    "try:\n",
    "    from rag_inference import RAGInferenceEngine\n",
    "except ImportError:\n",
    "    print(\"âŒ Please ensure all dependencies are installed: pip install -r requirements.txt\")\n",
    "    sys.exit(1)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"RAG System Deployment\")\n",
    "    parser.add_argument(\"--query\", type=str, required=True, help=\"Question to ask the RAG system\")\n",
    "    parser.add_argument(\"--config\", type=str, default=\"models/config.json\", help=\"Path to config file\")\n",
    "    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Enable verbose output\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Check if config exists\n",
    "    config_path = Path(args.config)\n",
    "    if not config_path.exists():\n",
    "        print(f\"âŒ Config file not found: {config_path}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    try:\n",
    "        # Initialize RAG engine\n",
    "        print(\"ðŸš€ Initializing RAG system...\")\n",
    "        engine = RAGInferenceEngine(str(config_path))\n",
    "        engine.initialize()\n",
    "        \n",
    "        # Process query\n",
    "        result = engine.query(args.query)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\\\n\" + \"=\"*60)\n",
    "        print(\"ðŸ“Š RAG SYSTEM RESPONSE\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"â“ Question: {result['question']}\")\n",
    "        print(f\"ðŸ’¡ Answer: {result['answer']}\")\n",
    "        \n",
    "        if args.verbose:\n",
    "            print(f\"\\\\nðŸ“š Retrieved {len(result['source_documents'])} source documents:\")\n",
    "            for i, doc in enumerate(result['source_documents'][:3]):\n",
    "                print(f\"  {i+1}. {doc['content'][:100]}...\")\n",
    "                if 'source' in doc['metadata']:\n",
    "                    print(f\"     Source: {doc['metadata']['source']}\")\n",
    "        \n",
    "        print(\"\\\\nâœ… Query processed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing query: {str(e)}\")\n",
    "        if args.verbose:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "    \n",
    "    with open(deployment_dir / \"deploy_rag.py\", \"w\") as f:\n",
    "        f.write(deployment_script)\n",
    "    \n",
    "    # Create standalone inference module\n",
    "    inference_module = '''\"\"\"\n",
    "Standalone RAG Inference Engine\n",
    "This module contains the complete RAG system for local deployment\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import warnings\n",
    "from typing import List, Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core imports\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from peft import PeftModel\n",
    "\n",
    "class RAGInferenceEngine:\n",
    "    \"\"\"Standalone RAG inference engine for local deployment\"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str):\n",
    "        self.config = self.load_config(config_path)\n",
    "        self.retriever = None\n",
    "        self.generator = None\n",
    "        self.tokenizer = None\n",
    "        print(f\"ðŸ—ï¸  RAG Engine initialized with config: {config_path}\")\n",
    "    \n",
    "    def load_config(self, config_path: str) -> Dict:\n",
    "        \"\"\"Load system configuration\"\"\"\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        return config\n",
    "    \n",
    "    def load_retriever(self):\n",
    "        \"\"\"Load the trained BGE retriever\"\"\"\n",
    "        print(\"ðŸ” Loading retriever...\")\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=self.config[\"retriever_model\"],\n",
    "            model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "        \n",
    "        # Load FAISS vector store\n",
    "        vector_store = FAISS.load_local(\n",
    "            self.config[\"vector_store_path\"], \n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        \n",
    "        self.retriever = vector_store.as_retriever(\n",
    "            search_kwargs={\"k\": self.config[\"retrieval_k\"]}\n",
    "        )\n",
    "    \n",
    "    def load_generator(self):\n",
    "        \"\"\"Load the fine-tuned Phi-2 generator\"\"\"\n",
    "        print(\"ðŸ§  Loading generator...\")\n",
    "        \n",
    "        try:\n",
    "            # Load base model with optimizations\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config[\"generator_model\"],\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                low_cpu_mem_usage=True\n",
    "            )\n",
    "            \n",
    "            # Load fine-tuned weights if they exist\n",
    "            generator_path = Path(self.config[\"generator_path\"])\n",
    "            if generator_path.exists():\n",
    "                self.generator = PeftModel.from_pretrained(base_model, str(generator_path))\n",
    "            else:\n",
    "                print(\"âš ï¸  Fine-tuned weights not found, using base model\")\n",
    "                self.generator = base_model\n",
    "            \n",
    "            # Load tokenizer\n",
    "            tokenizer_path = generator_path if generator_path.exists() else self.config[\"generator_model\"]\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(str(tokenizer_path), trust_remote_code=True)\n",
    "            \n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Error loading fine-tuned model: {e}\")\n",
    "            print(\"ðŸ”„ Falling back to base model...\")\n",
    "            \n",
    "            # Fallback to base model\n",
    "            self.generator = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config[\"generator_model\"],\n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.config[\"generator_model\"], \n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "    \n",
    "    def generate_answer(self, context: str, question: str) -> str:\n",
    "        \"\"\"Generate answer using the model\"\"\"\n",
    "        prompt = f\"Context: {context}\\\\n\\\\nQuestion: {question}\\\\n\\\\nAnswer:\"\n",
    "        \n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.to(self.generator.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.generator.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=150,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                repetition_penalty=1.1\n",
    "            )\n",
    "        \n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract answer part\n",
    "        if \"Answer:\" in generated_text:\n",
    "            answer = generated_text.split(\"Answer:\")[-1].strip()\n",
    "        else:\n",
    "            answer = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize all components\"\"\"\n",
    "        self.load_retriever()\n",
    "        self.load_generator()\n",
    "        print(\"âœ… RAG inference engine ready!\")\n",
    "    \n",
    "    def query(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process a query through the RAG pipeline\"\"\"\n",
    "        if not self.retriever or not self.generator:\n",
    "            raise RuntimeError(\"Engine not initialized. Call initialize() first.\")\n",
    "        \n",
    "        # Retrieve relevant documents\n",
    "        try:\n",
    "            retrieved_docs = self.retriever.get_relevant_documents(question)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Retrieval error: {e}\")\n",
    "            retrieved_docs = []\n",
    "        \n",
    "        # Combine contexts\n",
    "        context = \"\\\\n\\\\n\".join([doc.page_content for doc in retrieved_docs[:3]])\n",
    "        if not context:\n",
    "            context = \"No relevant context found.\"\n",
    "        \n",
    "        # Generate answer\n",
    "        answer = self.generate_answer(context, question)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"context\": context,\n",
    "            \"source_documents\": [\n",
    "                {\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"metadata\": doc.metadata\n",
    "                }\n",
    "                for doc in retrieved_docs\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Convenience function for quick usage\n",
    "def quick_query(question: str, config_path: str = \"models/config.json\") -> str:\n",
    "    \"\"\"Quick query function for simple usage\"\"\"\n",
    "    engine = RAGInferenceEngine(config_path)\n",
    "    engine.initialize()\n",
    "    result = engine.query(question)\n",
    "    return result[\"answer\"]\n",
    "'''\n",
    "    \n",
    "    with open(deployment_dir / \"rag_inference.py\", \"w\") as f:\n",
    "        f.write(inference_module)\n",
    "    \n",
    "    # Create Docker configuration\n",
    "    dockerfile = '''FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    gcc \\\\\n",
    "    g++ \\\\\n",
    "    git \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements and install Python dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application files\n",
    "COPY . .\n",
    "\n",
    "# Expose port for API (if implementing web interface)\n",
    "EXPOSE 8000\n",
    "\n",
    "# Default command\n",
    "CMD [\"python\", \"deploy_rag.py\", \"--query\", \"What is machine learning?\"]\n",
    "'''\n",
    "    \n",
    "    with open(deployment_dir / \"Dockerfile\", \"w\") as f:\n",
    "        f.write(dockerfile)\n",
    "    \n",
    "    # Create usage documentation\n",
    "    usage_docs = '''# RAG System Deployment Guide\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. **Install Dependencies:**\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "2. **Run a Query:**\n",
    "   ```bash\n",
    "   python deploy_rag.py --query \"What is machine learning?\"\n",
    "   ```\n",
    "\n",
    "3. **Python Usage:**\n",
    "   ```python\n",
    "   from rag_inference import RAGInferenceEngine\n",
    "   \n",
    "   engine = RAGInferenceEngine(\"models/config.json\")\n",
    "   engine.initialize()\n",
    "   result = engine.query(\"Your question here\")\n",
    "   print(result[\"answer\"])\n",
    "   ```\n",
    "\n",
    "## System Requirements\n",
    "\n",
    "- **Memory:** 8GB+ RAM recommended\n",
    "- **GPU:** Optional but recommended (CUDA-compatible)\n",
    "- **Storage:** 2GB+ for models and indices\n",
    "- **Python:** 3.8+\n",
    "\n",
    "## Docker Deployment\n",
    "\n",
    "1. **Build Image:**\n",
    "   ```bash\n",
    "   docker build -t rag-system .\n",
    "   ```\n",
    "\n",
    "2. **Run Container:**\n",
    "   ```bash\n",
    "   docker run -it rag-system python deploy_rag.py --query \"Your question\"\n",
    "   ```\n",
    "\n",
    "## Configuration\n",
    "\n",
    "Edit `models/config.json` to customize:\n",
    "- Model paths\n",
    "- Retrieval parameters\n",
    "- Generation settings\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "- **CUDA Issues:** Install appropriate PyTorch version for your CUDA version\n",
    "- **Memory Errors:** Reduce batch sizes or use CPU-only mode\n",
    "- **Model Not Found:** Ensure all model files are in correct directories\n",
    "\n",
    "## Performance Tips\n",
    "\n",
    "- Use GPU for faster inference\n",
    "- Increase `retrieval_k` for more context\n",
    "- Adjust generation parameters for quality vs speed\n",
    "'''\n",
    "    \n",
    "    with open(deployment_dir / \"README.md\", \"w\") as f:\n",
    "        f.write(usage_docs)\n",
    "    \n",
    "    print(f\"ðŸ“¦ Deployment package created at: {deployment_dir}/\")\n",
    "    print(\"âœ… Ready for production deployment!\")\n",
    "    \n",
    "    return deployment_dir\n",
    "\n",
    "# Create deployment package\n",
    "deployment_package = create_deployment_package()\n",
    "\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ðŸŽ® SECTION 10: INTERACTIVE DEMO & TESTING\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def run_interactive_demo():\n",
    "    \"\"\"Run an interactive demo of the RAG system\"\"\"\n",
    "    print(\"\\n\" + \"ðŸŽ®\" * 20)\n",
    "    print(\"INTERACTIVE RAG DEMO\")\n",
    "    print(\"ðŸŽ®\" * 20)\n",
    "    \n",
    "    # Prepare a simple demo with the training data\n",
    "    print(\"ðŸŽ¯ Setting up demo environment...\")\n",
    "    \n",
    "    # Create a simple query interface\n",
    "    demo_questions = [\n",
    "        \"What is machine learning?\",\n",
    "        \"How do neural networks work?\",\n",
    "        \"What is deep learning?\",\n",
    "        \"What are the differences between supervised and unsupervised learning?\",\n",
    "        \"How does backpropagation work?\",\n",
    "        \"What is the purpose of activation functions?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\\\nðŸ“‹ Available demo questions:\")\n",
    "    for i, q in enumerate(demo_questions, 1):\n",
    "        print(f\"  {i}. {q}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"RAG SYSTEM PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # System statistics\n",
    "    print(f\"ðŸ“Š System Statistics:\")\n",
    "    print(f\"  â€¢ Documents processed: {len(chunked_documents)}\")\n",
    "    print(f\"  â€¢ Vector store size: {len(chunked_documents)} embeddings\")\n",
    "    print(f\"  â€¢ Retriever model: {bge_retriever.model_name}\")\n",
    "    print(f\"  â€¢ Generator model: {phi2_generator.model_name}\")\n",
    "    print(f\"  â€¢ Training examples: {len(raw_qa_data)}\")\n",
    "    \n",
    "    # Memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        gpu_allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        gpu_reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        \n",
    "        print(f\"\\\\nðŸ–¥ï¸  GPU Memory Usage:\")\n",
    "        print(f\"  â€¢ Total: {gpu_memory:.1f} GB\")\n",
    "        print(f\"  â€¢ Allocated: {gpu_allocated:.1f} GB\")\n",
    "        print(f\"  â€¢ Reserved: {gpu_reserved:.1f} GB\")\n",
    "        print(f\"  â€¢ Available: {gpu_memory - gpu_reserved:.1f} GB\")\n",
    "    \n",
    "    print(f\"\\\\nðŸ’¾ Export Locations:\")\n",
    "    print(f\"  â€¢ Trained models: ./trained_models/\")\n",
    "    print(f\"  â€¢ Exported system: {export_path}/\")\n",
    "    print(f\"  â€¢ Deployment package: {deployment_package}/\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Run interactive demo\n",
    "demo_success = run_interactive_demo()\n",
    "\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ðŸ SECTION 11: FINAL SUMMARY & NEXT STEPS\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\\\n\" + \"ðŸ\" * 30)\n",
    "print(\"RAG SYSTEM TRAINING COMPLETE!\")\n",
    "print(\"ðŸ\" * 30)\n",
    "\n",
    "print(f\"\"\"\n",
    "âœ… **TRAINING COMPLETED SUCCESSFULLY!**\n",
    "\n",
    "ðŸŽ¯ **What was accomplished:**\n",
    "   â€¢ BGE-small-en-v1.5 retriever configured and indexed\n",
    "   â€¢ Phi-2 generator fine-tuned with LoRA\n",
    "   â€¢ FAISS vector store created with {len(chunked_documents)} document chunks\n",
    "   â€¢ Complete RAG pipeline assembled and tested\n",
    "   â€¢ Models exported for production deployment\n",
    "\n",
    "ðŸ“¦ **Generated Assets:**\n",
    "   â€¢ `trained_models/` - Raw trained model files\n",
    "   â€¢ `{export_path}/` - Complete exportable system\n",
    "   â€¢ `{deployment_package}/` - Production deployment package\n",
    "\n",
    "ðŸš€ **Next Steps:**\n",
    "   1. **Test Locally:** Use the inference engine to test queries\n",
    "   2. **Deploy:** Use the deployment package for production\n",
    "   3. **Scale:** Add more documents to the vector store\n",
    "   4. **Optimize:** Fine-tune parameters for your specific use case\n",
    "\n",
    "ðŸ”§ **Quick Usage:**\n",
    "   ```python\n",
    "   # Load and use the system\n",
    "   from rag_inference import RAGInferenceEngine\n",
    "   \n",
    "   engine = RAGInferenceEngine(\"exported_rag_system/config.json\")\n",
    "   engine.initialize()\n",
    "   \n",
    "   result = engine.query(\"Your question here\")\n",
    "   print(result[\"answer\"])\n",
    "   ```\n",
    "\n",
    "ðŸ“š **Documentation:**\n",
    "   â€¢ All code is thoroughly commented\n",
    "   â€¢ README files included in export directories\n",
    "   â€¢ Deployment guide available in deployment package\n",
    "\n",
    "âš¡ **Performance Notes:**\n",
    "   â€¢ System optimized for T4 GPU (Colab free tier)\n",
    "   â€¢ Uses 4-bit quantization and LoRA for memory efficiency\n",
    "   â€¢ FAISS provides fast similarity search\n",
    "   â€¢ Ready for production workloads\n",
    "\n",
    "ðŸŽ‰ **SUCCESS! Your RAG system is ready to deploy!**\n",
    "\"\"\")\n",
    "\n",
    "# Clean up GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\\\nðŸ”„ Memory cleanup completed\")\n",
    "print(\"ðŸŽ¯ RAG Training Pipeline Finished Successfully!\")\n",
    "\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ðŸ“š BONUS: ADDITIONAL UTILITIES & HELPERS\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def create_evaluation_suite():\n",
    "    \"\"\"Create evaluation utilities for the RAG system\"\"\"\n",
    "    \n",
    "    eval_code = '''\n",
    "\"\"\"\n",
    "RAG System Evaluation Suite\n",
    "Utilities for evaluating RAG system performance\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class RAGEvaluator:\n",
    "    \"\"\"Evaluate RAG system performance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    def evaluate_retrieval(self, questions: List[str], retrieved_docs: List[List[str]], \n",
    "                          ground_truth_docs: List[List[str]]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate retrieval quality\"\"\"\n",
    "        # Implementation for retrieval metrics\n",
    "        pass\n",
    "    \n",
    "    def evaluate_generation(self, questions: List[str], generated_answers: List[str], \n",
    "                           ground_truth_answers: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate generation quality using semantic similarity\"\"\"\n",
    "        \n",
    "        # Encode answers\n",
    "        gen_embeddings = self.embedding_model.encode(generated_answers)\n",
    "        gt_embeddings = self.embedding_model.encode(ground_truth_answers)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = []\n",
    "        for gen_emb, gt_emb in zip(gen_embeddings, gt_embeddings):\n",
    "            sim = cosine_similarity([gen_emb], [gt_emb])[0][0]\n",
    "            similarities.append(sim)\n",
    "        \n",
    "        return {\n",
    "            \"avg_semantic_similarity\": np.mean(similarities),\n",
    "            \"min_similarity\": np.min(similarities),\n",
    "            \"max_similarity\": np.max(similarities)\n",
    "        }\n",
    "    \n",
    "    def run_full_evaluation(self, rag_engine, test_data: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"Run comprehensive evaluation\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for item in test_data:\n",
    "            question = item['question']\n",
    "            result = rag_engine.query(question)\n",
    "            \n",
    "            evaluation = {\n",
    "                \"question\": question,\n",
    "                \"generated_answer\": result['answer'],\n",
    "                \"ground_truth\": item.get('answer', ''),\n",
    "                \"retrieved_docs_count\": len(result['source_documents']),\n",
    "                \"context_length\": len(result['context'])\n",
    "            }\n",
    "            results.append(evaluation)\n",
    "        \n",
    "        return {\n",
    "            \"individual_results\": results,\n",
    "            \"summary_stats\": self._calculate_summary_stats(results)\n",
    "        }\n",
    "    \n",
    "    def _calculate_summary_stats(self, results: List[Dict]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate summary statistics\"\"\"\n",
    "        return {\n",
    "            \"total_queries\": len(results),\n",
    "            \"avg_context_length\": np.mean([r['context_length'] for r in results]),\n",
    "            \"avg_retrieved_docs\": np.mean([r['retrieved_docs_count'] for r in results])\n",
    "        }\n",
    "    '''\n",
    "    \n",
    "    eval_dir = Path(\"evaluation_suite\")\n",
    "    eval_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    with open(eval_dir / \"rag_evaluator.py\", \"w\") as f:\n",
    "        f.write(eval_code)\n",
    "    \n",
    "    # Create sample evaluation script\n",
    "    eval_script = '''\n",
    "\"\"\"\n",
    "Sample evaluation script\n",
    "\"\"\"\n",
    "\n",
    "from rag_inference import RAGInferenceEngine\n",
    "from rag_evaluator import RAGEvaluator\n",
    "\n",
    "def main():\n",
    "    # Load RAG system\n",
    "    engine = RAGInferenceEngine(\"../exported_rag_system/config.json\")\n",
    "    engine.initialize()\n",
    "    \n",
    "    # Load evaluator\n",
    "    evaluator = RAGEvaluator()\n",
    "    \n",
    "    # Sample test data\n",
    "    test_data = [\n",
    "        {\n",
    "            \"question\": \"What is machine learning?\",\n",
    "            \"answer\": \"Machine learning is a subset of AI that enables systems to learn from data.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Run evaluation\n",
    "    results = evaluator.run_full_evaluation(engine, test_data)\n",
    "    print(\"Evaluation Results:\", results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    '''\n",
    "    \n",
    "    with open(eval_dir / \"run_evaluation.py\", \"w\") as f:\n",
    "        f.write(eval_script)\n",
    "    \n",
    "    print(f\"ðŸ“Š Evaluation suite created at: {eval_dir}/\")\n",
    "\n",
    "# Create evaluation suite\n",
    "create_evaluation_suite()\n",
    "\n",
    "print(\"\\\\nðŸŽŠ ALL COMPONENTS SUCCESSFULLY CREATED!\")\n",
    "print(\"ðŸ“ Check the generated directories for all files and utilities.\")\n",
    "print(\"ðŸš€ Your RAG system is production-ready!\")\n",
    "\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ðŸ”š END OF NOTEBOOK\n",
    "#â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enginuity-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
