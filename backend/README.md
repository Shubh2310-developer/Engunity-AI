
üéØ RAG Training Workflow with Computer Science Dataset
Great! You've downloaded a Computer Science Q&A dataset. Here's the updated workflow specifically tailored for this dataset:

üìã Phase 1: Dataset Analysis & Preparation
Files to Create/Edit:
backend/data/training/dataset_analyzer.py
Analyze the downloaded CS dataset structure
Extract question types, topics, difficulty levels
Map to your document domains (code, research, technical docs)
backend/data/training/cs_preprocessor.py
Clean and standardize CS Q&A format
Filter relevant questions for your SaaS domains
Extract technical terms and code snippets
backend/data/training/domain_mapper.py
Map CS topics to your SaaS modules:
Programming ‚Üí Code Assistant
Algorithms ‚Üí Research Tools
Data Structures ‚Üí Data Analysis
Software Engineering ‚Üí Document Q&A
Dataset Structure Analysis:
backend/data/training/
‚îú‚îÄ‚îÄ kaggle_cs_dataset/          # Original downloaded dataset
‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îú‚îÄ‚îÄ filtered_qa_pairs.jsonl    # CS questions relevant to your SaaS
‚îÇ   ‚îú‚îÄ‚îÄ code_questions.jsonl       # Programming/coding questions
‚îÇ   ‚îú‚îÄ‚îÄ theory_questions.jsonl     # Theoretical CS concepts
‚îÇ   ‚îú‚îÄ‚îÄ algorithm_questions.jsonl  # Algorithm and data structure Q&A
‚îÇ   ‚îî‚îÄ‚îÄ mixed_difficulty.jsonl     # Questions by difficulty level
‚îî‚îÄ‚îÄ synthetic/
    ‚îú‚îÄ‚îÄ document_based_qa.jsonl     # Generated from your docs + CS knowledge
    ‚îî‚îÄ‚îÄ hybrid_questions.jsonl     # Mix of CS concepts + user documents
üìä Phase 2: CS-Specific Data Preparation
Files to Create/Edit:
backend/app/services/rag/cs_data_processor.py
Process CS questions into embedding-friendly format
Extract code snippets and technical terminology
Create domain-specific negative samples
backend/app/services/rag/cs_synthetic_generator.py
Use Groq API + CS dataset to generate synthetic pairs
Create questions that bridge CS concepts with user documents
Generate multi-hop reasoning questions
backend/data/training/cs_evaluation_set.py
Create CS-specific evaluation benchmarks
Programming comprehension tests
Algorithm explanation tasks
Code documentation Q&A
Data Processing Strategy:
Filter Relevant Questions: ~60% of CS dataset (focus on practical topics)
Augment with User Data: Mix CS knowledge with uploaded documents
Create Hierarchical Topics: Basic ‚Üí Intermediate ‚Üí Advanced CS concepts
üß† Phase 3: CS-Domain Embedding Training
Files to Create/Edit:
backend/app/services/rag/cs_embedding_trainer.py
Fine-tune bge-small-en-v1.5 on CS + user document pairs
Technical term understanding improvement
Code-text semantic alignment
backend/app/models/cs_embedding_config.py
CS-specific training hyperparameters
Technical vocabulary enhancement
Code snippet embedding optimization
backend/app/services/rag/cs_contrastive_learning.py
Hard negative mining from CS topics
Cross-domain contrastive learning (theory ‚Üî practice)
Code similarity vs. conceptual similarity
Training Data Composition:
50%: CS Dataset (filtered & processed)
30%: User document Q&A pairs
20%: Synthetic hybrid questions
üîç Phase 4: CS-Enhanced Vector Store
Files to Create/Edit:
backend/vector_store/cs_faiss_manager.py
Multi-index setup for different CS domains
Code embedding index vs. theory index
Cross-domain retrieval optimization
backend/app/services/rag/cs_retriever.py
CS-aware query understanding
Technical term expansion
Code-documentation hybrid search
backend/app/services/rag/cs_query_processor.py
Detect question type (code, theory, mixed)
Route to appropriate retrieval strategy
Handle technical abbreviations and acronyms
ü§ñ Phase 5: CS-Aware Response Generation
Files to Create/Edit:
backend/app/services/rag/cs_generator.py
CS-specific prompt templates for Groq API
Code explanation and documentation generation
Technical concept simplification
backend/app/services/rag/cs_prompt_templates.py
Programming question templates
Algorithm explanation prompts
Code review and documentation prompts
Theory-to-practice connection prompts
backend/app/services/rag/cs_response_validator.py
Validate technical accuracy
Check code syntax and logic
Ensure appropriate complexity level
üìà Phase 6: CS-Specific Evaluation
Files to Create/Edit:
backend/tests/rag/cs_evaluation.py
CS knowledge accuracy tests
Code comprehension benchmarks
Technical explanation quality metrics
backend/app/services/rag/cs_feedback_analyzer.py
Domain-specific feedback collection
Technical accuracy user ratings
Learning progression tracking
üîÑ Complete CS-Enhanced Training Workflow
Week 1: Dataset Integration
bash
# Primary files to work on:
backend/data/training/dataset_analyzer.py
backend/data/training/cs_preprocessor.py
backend/data/training/domain_mapper.py
Tasks:

Analyze CS dataset structure and content
Filter 15K-20K relevant CS Q&A pairs
Map CS topics to your SaaS modules
Create domain-specific training splits
Week 2: Data Augmentation
bash
# Primary files to work on:
backend/app/services/rag/cs_data_processor.py
backend/app/services/rag/cs_synthetic_generator.py
backend/data/training/cs_evaluation_set.py
Tasks:

Process CS questions for embedding training
Generate 10K synthetic hybrid questions
Create CS-specific evaluation benchmarks
Build negative sampling strategy
Week 3: Embedding Training
bash
# Primary files to work on:
backend/app/services/rag/cs_embedding_trainer.py
backend/app/models/cs_embedding_config.py
backend/app/services/rag/cs_contrastive_learning.py
Tasks:

Fine-tune embeddings on CS + user data
Optimize for technical term understanding
Implement code-text semantic alignment
Validate on CS retrieval tasks
Week 4: Enhanced Retrieval
bash
# Primary files to work on:
backend/vector_store/cs_faiss_manager.py
backend/app/services/rag/cs_retriever.py
backend/app/services/rag/cs_query_processor.py
Tasks:

Build CS-aware vector store
Implement domain-specific retrieval
Add technical query processing
Test cross-domain search
Week 5: Generation & Validation
bash
# Primary files to work on:
backend/app/services/rag/cs_generator.py
backend/app/services/rag/cs_prompt_templates.py
backend/app/services/rag/cs_response_validator.py
Tasks:

Integrate CS-aware generation
Create technical prompt templates
Implement response validation
Test on CS evaluation set
Week 6: Deployment & Monitoring
bash
# Primary files to work on:
backend/tests/rag/cs_evaluation.py
backend/app/services/rag/cs_feedback_analyzer.py
Tasks:

Deploy CS-enhanced RAG system
Set up CS-specific monitoring
Implement domain feedback collection
Monitor technical accuracy metrics
üéØ CS Dataset Optimization for 6GB VRAM
Memory-Efficient Training Strategy:
Batch Processing: Process CS dataset in chunks of 1K questions
Progressive Training: Start with basic CS concepts, add complexity
Domain Rotation: Alternate between CS topics during training
Checkpoint Strategy: Save models after each CS domain
Training Data Prioritization:
High Priority: Programming, Data Structures, Algorithms (60%)
Medium Priority: Software Engineering, Systems (25%)
Low Priority: Theoretical CS, Advanced Math (15%)
üìä CS-Enhanced Success Metrics
Technical Accuracy:
CS Knowledge: >90% accuracy on basic CS concepts
Code Understanding: >85% accuracy in code explanation
Cross-Domain: >80% accuracy linking theory to practice
User Experience:
Programming Help: 70% of users find code explanations helpful
Learning Support: 60% improvement in CS concept understanding
Research Assistance: 50% faster algorithm research
Integration with Existing Modules:
Code Assistant Enhancement:
Update: frontend/src/app/(dashboard)/code/page.tsx
Add: CS concept explanations alongside code generation
Add: Algorithm complexity analysis
Research Module Enhancement:
Update: frontend/src/app/(dashboard)/research/page.tsx
Add: CS paper analysis and algorithm research
Add: Technical bibliography generation
Document Q&A Enhancement:
Update: frontend/src/app/(dashboard)/documents/[id]/qa/page.tsx
Add: Technical document understanding
Add: Code documentation generation
This CS-enhanced workflow will significantly improve your RAG system's ability to handle technical questions and provide more accurate, contextual responses for computer science and programming-related queries.




give me commands to make the new files on linux

Edit
üêß Linux Commands to Create RAG Training File Structure
üìÅ Phase 1: Create Main Directory Structure
bash
# Navigate to your project root
cd /path/to/engunity-ai

# Create main training directories
mkdir -p backend/data/training/{kaggle_cs_dataset,processed,synthetic}
mkdir -p backend/tests/rag
mkdir -p backend/app/services/rag
mkdir -p backend/app/models
mkdir -p backend/vector_store
üìä Phase 2: Create Dataset Analysis Files
bash
# Dataset analysis and preprocessing files
touch backend/data/training/dataset_analyzer.py
touch backend/data/training/cs_preprocessor.py
touch backend/data/training/domain_mapper.py
touch backend/data/training/cs_evaluation_set.py

# Create processed data files
touch backend/data/training/processed/filtered_qa_pairs.jsonl
touch backend/data/training/processed/code_questions.jsonl
touch backend/data/training/processed/theory_questions.jsonl
touch backend/data/training/processed/algorithm_questions.jsonl
touch backend/data/training/processed/mixed_difficulty.jsonl

# Create synthetic data files
touch backend/data/training/synthetic/document_based_qa.jsonl
touch backend/data/training/synthetic/hybrid_questions.jsonl
üß† Phase 3: Create RAG Service Files
bash
# Core RAG service files
touch backend/app/services/rag/__init__.py
touch backend/app/services/rag/data_collector.py
touch backend/app/services/rag/preprocessor.py
touch backend/app/services/rag/synthetic_generator.py
touch backend/app/services/rag/embedding_trainer.py
touch backend/app/services/rag/training_pipeline.py
touch backend/app/services/rag/indexer.py
touch backend/app/services/rag/retriever.py
touch backend/app/services/rag/generator.py
touch backend/app/services/rag/prompt_templates.py
touch backend/app/services/rag/response_processor.py
touch backend/app/services/rag/evaluator.py
touch backend/app/services/rag/feedback_collector.py
üî¨ Phase 4: Create CS-Specific Files
bash
# CS-specific RAG files
touch backend/app/services/rag/cs_data_processor.py
touch backend/app/services/rag/cs_synthetic_generator.py
touch backend/app/services/rag/cs_embedding_trainer.py
touch backend/app/services/rag/cs_contrastive_learning.py
touch backend/app/services/rag/cs_retriever.py
touch backend/app/services/rag/cs_query_processor.py
touch backend/app/services/rag/cs_generator.py
touch backend/app/services/rag/cs_prompt_templates.py
touch backend/app/services/rag/cs_response_validator.py
touch backend/app/services/rag/cs_feedback_analyzer.py
‚öôÔ∏è Phase 5: Create Model Configuration Files
bash
# Model configuration files
touch backend/app/models/embedding_config.py
touch backend/app/models/cs_embedding_config.py
üóÑÔ∏è Phase 6: Create Vector Store Files
bash
# Update existing and create new vector store files
touch backend/vector_store/cs_faiss_manager.py
touch backend/vector_store/config.json
touch backend/vector_store/embeddings/.gitkeep
touch backend/vector_store/indices/.gitkeep
üß™ Phase 7: Create Test Files
bash
# RAG testing files
touch backend/tests/rag/__init__.py
touch backend/tests/rag/test_embedding.py
touch backend/tests/rag/test_retrieval.py
touch backend/tests/rag/test_generation.py
touch backend/tests/rag/evaluation.py
touch backend/tests/rag/benchmarks.py
touch backend/tests/rag/cs_evaluation.py
üìÑ Phase 8: Create Configuration and Script Files
bash
# Training configuration files
touch backend/config/rag_config.yaml
touch backend/config/cs_training_config.yaml

# Training scripts
mkdir -p backend/scripts/rag
touch backend/scripts/rag/train_embeddings.py
touch backend/scripts/rag/build_index.py
touch backend/scripts/rag/evaluate_rag.py
touch backend/scripts/rag/deploy_models.py
üîÑ Phase 9: Create Data Processing Scripts
bash
# Data processing and utility scripts
touch backend/scripts/rag/process_cs_dataset.py
touch backend/scripts/rag/generate_synthetic_data.py
touch backend/scripts/rag/validate_training_data.py
touch backend/scripts/rag/export_models.py
üìö Phase 10: Create Documentation Files
bash
# RAG-specific documentation
mkdir -p docs/rag
touch docs/rag/training_guide.md
touch docs/rag/cs_dataset_guide.md
touch docs/rag/deployment_guide.md
touch docs/rag/evaluation_metrics.md
üéØ Complete One-Shot Command
bash
# Execute all at once (copy and paste this entire block)
cd /path/to/engunity-ai && \
mkdir -p backend/data/training/{kaggle_cs_dataset,processed,synthetic} && \
mkdir -p backend/tests/rag && \
mkdir -p backend/app/services/rag && \
mkdir -p backend/app/models && \
mkdir -p backend/vector_store && \
mkdir -p backend/scripts/rag && \
mkdir -p backend/config && \
mkdir -p docs/rag && \
touch backend/data/training/{dataset_analyzer.py,cs_preprocessor.py,domain_mapper.py,cs_evaluation_set.py} && \
touch backend/data/training/processed/{filtered_qa_pairs.jsonl,code_questions.jsonl,theory_questions.jsonl,algorithm_questions.jsonl,mixed_difficulty.jsonl} && \
touch backend/data/training/synthetic/{document_based_qa.jsonl,hybrid_questions.jsonl} && \
touch backend/app/services/rag/{__init__.py,data_collector.py,preprocessor.py,synthetic_generator.py,embedding_trainer.py,training_pipeline.py,indexer.py,retriever.py,generator.py,prompt_templates.py,response_processor.py,evaluator.py,feedback_collector.py} && \
touch backend/app/services/rag/{cs_data_processor.py,cs_synthetic_generator.py,cs_embedding_trainer.py,cs_contrastive_learning.py,cs_retriever.py,cs_query_processor.py,cs_generator.py,cs_prompt_templates.py,cs_response_validator.py,cs_feedback_analyzer.py} && \
touch backend/app/models/{embedding_config.py,cs_embedding_config.py} && \
touch backend/vector_store/{cs_faiss_manager.py,config.json} && \
touch backend/vector_store/embeddings/.gitkeep && \
touch backend/vector_store/indices/.gitkeep && \
touch backend/tests/rag/{__init__.py,test_embedding.py,test_retrieval.py,test_generation.py,evaluation.py,benchmarks.py,cs_evaluation.py} && \
touch backend/config/{rag_config.yaml,cs_training_config.yaml} && \
touch backend/scripts/rag/{train_embeddings.py,build_index.py,evaluate_rag.py,deploy_models.py,process_cs_dataset.py,generate_synthetic_data.py,validate_training_data.py,export_models.py} && \
touch docs/rag/{training_guide.md,cs_dataset_guide.md,deployment_guide.md,evaluation_metrics.md} && \
echo "‚úÖ All RAG training files created successfully!"
üîç Verify File Creation
bash
# Check directory structure
tree backend/data/training/
tree backend/app/services/rag/
tree backend/tests/rag/

# Count created files
find backend/data/training/ -name "*.py" -o -name "*.jsonl" | wc -l
find backend/app/services/rag/ -name "*.py" | wc -l
find backend/tests/rag/ -name "*.py" | wc -l
üìù Set Proper Permissions
bash
# Make scripts executable
chmod +x backend/scripts/rag/*.py

# Set proper file permissions
find backend/data/training/ -type f -name "*.py" -exec chmod 644 {} \;
find backend/app/services/rag/ -type f -name "*.py" -exec chmod 644 {} \;
find backend/tests/rag/ -type f -name "*.py" -exec chmod 644 {} \;
üóÇÔ∏è Move Downloaded Dataset
bash
# Move your Kaggle dataset to the proper location
# Replace /path/to/downloaded/dataset with your actual download path
mv /path/to/downloaded/dataset/* backend/data/training/kaggle_cs_dataset/

# Or if you know the exact path from kagglehub:
# mv ~/.cache/kagglehub/datasets/elemam/quetions-on-computer-science-llms/* backend/data/training/kaggle_cs_dataset/
üìä Initialize Git Tracking
bash
# Add new files to git
git add backend/data/training/
git add backend/app/services/rag/
git add backend/tests/rag/
git add backend/scripts/rag/
git add backend/config/
git add docs/rag/

# Create initial commit
git commit -m "feat: add RAG training pipeline structure for CS dataset integration"
üéØ Quick Start Commands
bash
# After creating all files, start with these commands to begin development:

# 1. Analyze the CS dataset
python backend/data/training/dataset_analyzer.py

# 2. Process the CS data
python backend/scripts/rag/process_cs_dataset.py

# 3. Generate synthetic training data
python backend/scripts/rag/generate_synthetic_data.py

# 4. Validate training data quality
python backend/scripts/rag/validate_training_data.py

# 5. Start embedding training
python backend/scripts/rag/train_embeddings.py
All files are now created and ready for implementation! Start with Phase 1 (Dataset Analysis) and work through each file systematically.





üìã RAG Training Files - Proper Implementation Order
WEEK 1: Dataset Foundation
Day 1-2: Dataset Analysis & Setup
1. backend/data/training/dataset_analyzer.py ‚≠ê START HERE
bash# Purpose: Understand your CS dataset structure
# Priority: CRITICAL - Must complete before anything else
2. backend/config/rag_config.yaml
bash# Purpose: Set up configuration parameters
# Priority: HIGH - Needed for all subsequent files
3. backend/config/cs_training_config.yaml
bash# Purpose: CS-specific training configurations
# Priority: HIGH - Defines training hyperparameters
Day 3-4: Data Preprocessing
4. backend/data/training/cs_preprocessor.py
bash# Purpose: Clean and standardize CS dataset
# Depends on: dataset_analyzer.py results
5. backend/data/training/domain_mapper.py
bash# Purpose: Map CS topics to your SaaS modules
# Depends on: cs_preprocessor.py output
Day 5-7: Data Processing Pipeline
6. backend/app/services/rag/data_collector.py
bash# Purpose: Collect existing user interaction data
# Works parallel with CS data processing
7. backend/scripts/rag/process_cs_dataset.py
bash# Purpose: Main script to process entire CS dataset
# Depends on: cs_preprocessor.py, domain_mapper.py

WEEK 2: Data Augmentation & Preparation
Day 8-10: Synthetic Data Generation
8. backend/app/services/rag/cs_data_processor.py
bash# Purpose: Process CS questions for training
# Depends on: processed CS dataset
9. backend/app/services/rag/synthetic_generator.py
bash# Purpose: Generate basic synthetic Q&A pairs
# Depends on: Groq API setup
10. backend/app/services/rag/cs_synthetic_generator.py
bash# Purpose: Generate CS-specific synthetic data
# Depends on: synthetic_generator.py base implementation
Day 11-14: Training Data Preparation
11. backend/scripts/rag/generate_synthetic_data.py
bash# Purpose: Main script for synthetic data generation
# Depends on: All synthetic generators
12. backend/data/training/cs_evaluation_set.py
bash# Purpose: Create evaluation benchmarks
# Depends on: Processed CS data
13. backend/scripts/rag/validate_training_data.py
bash# Purpose: Validate data quality before training
# Depends on: All data preparation files

WEEK 3: Embedding Model Training
Day 15-17: Training Setup
14. backend/app/models/embedding_config.py
bash# Purpose: Base embedding model configuration
# Priority: Required before training
15. backend/app/models/cs_embedding_config.py
bash# Purpose: CS-specific embedding configurations
# Depends on: embedding_config.py
16. backend/app/services/rag/preprocessor.py
bash# Purpose: General text preprocessing for embeddings
# Needed for training pipeline
Day 18-21: Training Implementation
17. backend/app/services/rag/cs_contrastive_learning.py
bash# Purpose: Contrastive learning implementation
# Core training algorithm
18. backend/app/services/rag/embedding_trainer.py
bash# Purpose: Base embedding training logic
# Depends on: preprocessor.py, embedding_config.py
19. backend/app/services/rag/cs_embedding_trainer.py
bash# Purpose: CS-specific embedding training
# Depends on: embedding_trainer.py, cs_contrastive_learning.py
20. backend/app/services/rag/training_pipeline.py
bash# Purpose: Complete training pipeline orchestration
# Depends on: All training components
21. backend/scripts/rag/train_embeddings.py
bash# Purpose: Main training execution script
# Depends on: training_pipeline.py

WEEK 4: Vector Store & Retrieval
Day 22-24: Vector Store Setup
22. backend/vector_store/config.json
bash# Purpose: Vector store configuration
# Simple configuration file
23. backend/app/services/rag/indexer.py
bash# Purpose: Document indexing logic
# Depends on: Trained embeddings
24. backend/vector_store/cs_faiss_manager.py
bash# Purpose: CS-specific FAISS index management
# Depends on: indexer.py, trained embeddings
Day 25-28: Retrieval System
25. backend/app/services/rag/cs_query_processor.py
bash# Purpose: Process CS-specific queries
# Core retrieval component
26. backend/app/services/rag/retriever.py
bash# Purpose: Base retrieval logic
# Depends on: vector store setup
27. backend/app/services/rag/cs_retriever.py
bash# Purpose: CS-enhanced retrieval
# Depends on: retriever.py, cs_query_processor.py
28. backend/scripts/rag/build_index.py
bash# Purpose: Build and deploy vector indices
# Depends on: All retrieval components

WEEK 5: Response Generation
Day 29-31: Generation Setup
29. backend/app/services/rag/prompt_templates.py
bash# Purpose: Base prompt templates
# Required for generation
30. backend/app/services/rag/cs_prompt_templates.py
bash# Purpose: CS-specific prompt templates
# Depends on: prompt_templates.py
31. backend/app/services/rag/response_processor.py
bash# Purpose: Process and validate responses
# Core generation component
Day 32-35: Generation Implementation
32. backend/app/services/rag/generator.py
bash# Purpose: Base response generation
# Depends on: prompt_templates.py, response_processor.py
33. backend/app/services/rag/cs_generator.py
bash# Purpose: CS-specific response generation
# Depends on: generator.py, cs_prompt_templates.py
34. backend/app/services/rag/cs_response_validator.py
bash# Purpose: Validate CS responses for accuracy
# Depends on: cs_generator.py

WEEK 6: Evaluation & Deployment
Day 36-38: Evaluation System
35. backend/app/services/rag/evaluator.py
bash# Purpose: Base evaluation metrics
# Required for testing
36. backend/tests/rag/evaluation.py
bash# Purpose: Evaluation test suite
# Depends on: evaluator.py
37. backend/tests/rag/cs_evaluation.py
bash# Purpose: CS-specific evaluation tests
# Depends on: evaluation.py
Day 39-42: Testing & Deployment
38. backend/tests/rag/test_embedding.py
bash# Purpose: Test embedding functionality
# Depends on: Trained embeddings
39. backend/tests/rag/test_retrieval.py
bash# Purpose: Test retrieval accuracy
# Depends on: Built indices
40. backend/tests/rag/test_generation.py
bash# Purpose: Test generation quality
# Depends on: All generation components
41. backend/scripts/rag/evaluate_rag.py
bash# Purpose: Full system evaluation script
# Depends on: All test files
42. backend/scripts/rag/deploy_models.py
bash# Purpose: Deploy trained models
# Final deployment script

ONGOING: Monitoring & Feedback
Implement After Initial Deployment
43. backend/app/services/rag/feedback_collector.py
bash# Purpose: Collect user feedback
# Implements after deployment
44. backend/app/services/rag/cs_feedback_analyzer.py
bash# Purpose: Analyze CS-specific feedback
# Depends on: feedback_collector.py
45. backend/tests/rag/benchmarks.py
bash# Purpose: Performance benchmarking
# Ongoing optimization tool

üìã Quick Start Checklist
Before You Begin:
bash# 1. Ensure your Kaggle CS dataset is in place
ls backend/data/training/kaggle_cs_dataset/

# 2. Set up your development environment
python -m venv rag_env
source rag_env/bin/activate
pip install -r requirements.txt

# 3. Configure your Groq API key
export GROQ_API_KEY="your_groq_api_key"
Critical Dependencies:

Week 1: Complete dataset analysis before ANY other work
Week 2: Don't start Week 3 without validated training data
Week 3: Must have working embeddings before vector store
Week 4: Need functional retrieval before generation
Week 5: Ensure generation works before evaluation
Week 6: Complete testing before deployment

Parallel Work Opportunities:

Documentation: Write docs files while implementing
Configuration: Update configs as you discover requirements
Testing: Write test files alongside implementation files

üéØ Start with dataset_analyzer.py and follow this exact order for best results!RetryClaude can make mistakes. Please double-check responses.


